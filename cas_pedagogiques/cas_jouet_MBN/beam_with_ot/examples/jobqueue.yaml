jobqueue:
  slurm:
     name: dask-worker
     # Dask worker options
     cores: 4                 # Total number of cores per job
     memory: "512 MB"                # Total amount of memory per job
     processes: 1                # Number of Python processes per job
     interface: ib0             # Network interface to use like eth0 or ib0
#     death-timeout: 60           # Number of seconds to wait if a worker can not find a scheduler
#     local-directory: null       # Location of fast local storage like /scratch or $TMPDIR
#     shared-temp-directory: null       # Shared directory currently used to dump temporary security objects for workers
#     extra: null                 # deprecated: use worker-extra-args
#     worker-extra-args: []       # Additional arguments to pass to `dask-worker`

     # SLURM resource manager options
#     shebang: "#!/usr/bin/env bash"
#     queue: regular
#     account: null
#     walltime: '00:30:00'
#     env-extra: null
#     job-script-prologue: []
#     job-cpu: null
#     job-mem: null
#     job-extra: null
#     job-extra-directives: ['--wckey=p127z:datascience', '--time=5', '--output=wrapper.log', '--error=wrapper_error.log']
#     job-directives-skip: []
#     log-directory: null

#     # Scheduler options
#     scheduler-options: {}


